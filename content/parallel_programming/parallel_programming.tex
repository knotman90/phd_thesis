
\chapter{Parallel Computing}
\label{ch:parallel_computing}

\section{Introduction}
Over the last two decades lot has changed regarding the way modern scientific applications are designed, written and executed especially in the field of data-analytics, modelling and simulation and visualization mainly because the size of the problems that scientist try to tackle nowadays are much bigger and because of the amount of available raw data that can be analyzed. 
Traditionally performance improvements in computer architecture have come from cramming ever more functional units onto silicon, increasing clock speeds and transistors number. Moore's law states that the number of transistors that can be placed inexpensively on an integrated circuit will double approximately every two years (See Figure \ref{fig:moore}). Coupled with increasing clock speeds CPU performance has until recently scaled likewise. But it is important to acknowledge that this trend cannot be sustained indefinitely or forever. Increased clock speed and transistor number require more power and consequently generate more heat. Although the trend for transistor densities has continued to steadily increase, clock speeds began slowing circa 2003 at 3 GHz. If we apply Mooreâ€™s law type thinking to clock-speed performance, we should be able to buy at least 10 GHz CPUs. However, the fastest CPU available at the time of writing is $\approx 4.0 GHz$.

\begin{figure}
\centering
\includegraphics[totalheight=0.5\textheight]{./images/parallel_programming/moore_law_large}
\includegraphics[scale=0.12]{./images/parallel_programming/moore_law2}
\caption{Moore's Law regarding CPU transistors number
history.}\label{mooreLaw}

\end{figure}




At same point the performance gaining fails to increase proportionally with the added effort in terms of transistors number or clock speed because efficient heat dissipation and increasing transistor resolution on the wafer, which is not far from from its physical limit (the atom scale),  becomes more important and challenging.
The heat emitted from the modern processor, measured in power density rivals the heat of a nuclear reactor core (see Figures \ref{fig:tempCPU} and \ref{fig:tempCPU_thermal})!
But the power demand did not stop in these year, and is not going to stop in the near future, here the necessity of relying heavily on parallel architectures, so today the dominating trend in commodity CPU architectures is multiple processing cores mounted on a single die operating at reduced clock speeds and sharing resources and memory. Today is normal to use multicore (2,4,8,12, up to 40) CPUs on a desktop PC at home at the point that is very uncommon to be able to buy a single core device.
Even smartphones are proper multicore machines; for instance, the popular mobile CPU \textit{Snapdragon 835} manufactured by Qualcomm is a $8 \times$ cores each of them with a clock speed up to $2.45$ GHz

\begin{figure}
\centering
\includegraphics[scale=0.7]{./images/parallel_programming/temperatureCPU}
\caption{Temperature density CPUs}\label{fig:tempCPU}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.6]{./images/parallel_programming/heat_cpu}
\caption{Thermal camera image of a modern CPU showing that heat is concentrated in a small part of the wafer.}\label{fig:tempCPU_thermal}
\end{figure}

Lot of effort has put during these years in order to try to mitigate and overcome the limits regarding the sequential computer architecture which may derive from its three main components:
\begin{enumerate}

\item  Processor

\item Memory

\item  Communication system (datapaths, usually buses)
\end{enumerate}

All three components present bottlenecks that limit the overall computing performance capability of a system. Caches, low-latency high bandwidth and small capacity memories, for example can hide latency of DRAM chips storing the fetched data and serving subsequent requests of the same memory location\footnote{The fraction of the data satisfied by the cache is called \textit{\textbf{hit rate}}.}. But one of the most important innovation that addresses these bottlenecks is multiplicity (in processor, memories and datapaths) that allows to extend the class of tractable problems with larger instances and more cases that can be handled. This multiplicity has been organized in several manners during the years giving birth to a variety of architectures.

\section{Architectures}
Lots of definitions and classifications have been proposed in years, mostly based on the adopted hardware configuration or logical approach in handling and implementing the parallelism, in order to categorize parallel systems.Among all, Flynn's taxonomy seems to be a well known and accepted one and is here introduced briefly.
\subsection{Classical classification - Flynn's taxonomy}
The classification is based on the notion of  \textit{stream of information}.
Two types of information flow into the processor: instructions and data.
Conceptually they can be separated into two independent streams. A coarse
classification can be made taking in account only the number of instructions and
 data streams that a parallel machine can manage (see figure \ref{fig:parallelClassification1}).
That's how Flynn's taxonomy\cite{Flynn1972} classifies machines: according to
whether they have one or more streams of each type.
\begin{figure}
\includegraphics[scale=0.28]{./images/parallel_programming/parallelClassification}
\caption[Flynn's Parallel architecture taxonomy]{Flynn's Parallel architecture taxonomy.}
\label{fig:parallelClassification1}
\end{figure}
\begin{description}
\item[SISD:] \textit{\textbf{Single}} instruction \textit{\textbf{Single}}
data.\hfill\\
No parallelism in either instruction or data streams. Each arithmetic
instruction initiates an operation on a data item taken from a single stream of data elements (e.g. mainframes). A single control unit fetches a single instruction from the memory.
\item[SIMD:] \textit{\textbf{Single}} instruction
\textit{\textbf{Multiple}} data. \hfill \\ Data parallelism. The same
instruction is executed on a batch of different data. The control unit is
responsible for fetching and interpreting instructions. When it encounters an
arithmetic or other data processing instruction, it broadcasts the instruction
to all processing elements (PE), which then all perform the same operation. For
example, the instruction might be \textit{add R3,R0.}. Each PE would add the
contents of its own internal register R3 to its own R0. (e.g. stream
processors\footnote{Vector processing is performed on an SIMD machine by distributing
elements of vectors across all data memories.}.) SSE and AVX extensions to the $x86$ processors family is an example of such parallelism. One single instruction can operate on up to $512$ byte of data (see Figure \ref{fig:SSEvectorization}). SIMD exploits data and spatial prallelism in a synchronous manner.

\begin{figure}
	\centering
	\includegraphics[scale=0.6]{./images/parallel_programming/vectorization_example}
	\caption{Example of vectorization with Intel SSE and extension.}
	\label{fig:SSEvectorization}
\end{figure}

\item[MISD:] \textit{\textbf{Multiple}} instruction \textit{\textbf{Single}} data. \hfill \\ 
Multiple instruction operating on the same data stream  (See Figure \ref{fig:MISD}). It is a class of system very unusual, mostly for fault-tolerance reasons. No machines in this category have been commercially successful or had any impact on computational science. 
An type of of computer  that fits the MISD description is the so called \textit{systolic array} which consists of a network of pipelined primitive computing nodes or processors. 
\begin{figure}
	\centering
	\includegraphics[scale=0.30]{./images/parallel_programming/MISD}
	\caption{MISD machine schematization. Each processor $CU_i$ has its own instruction flow and all operates on the same data.}
	\label{fig:MISD}
\end{figure}
 
\item[MIMD:] \textit{\textbf{Multiple}} instruction

\textit{\textbf{Multiple}} data. \hfill \\ Multiple instruction operating
independently on multiple data streams (e.g. most modern computers). A MIMD machine is an example of a true multiprocessor and are often employed to ro perform the so called \textit{Single Program Multiple Data} computation where each independent processor executes the same program.
MIMD architecture can be further divided by by their memory layout and organization:
\begin{description}
	\item [Shared Memory (modern CPUs)] where each processors shares the same memory address space and are interconnected by a shared buses. 	Shared memory architecture ar usually shipped as \textit{Symmetric Multi Processors} (SMP) since each of them is usually identical in computational and access to resources capabilities, and the OS kernel can run on any of them in contrast to \textit{Asymettric Multiprocessor} where there is a master-slave relationship  among the group of processors. According to whether subsets of processors have a dedicated and private memory module we can categorize SM architectures in:	
	\begin{itemize}
		\item UMA (Uniform Memory Access) : Identical processors with equal
		access time to memory (see figure \ref{fig:UMA_NUMA}),
		sometimes called CC-UMA acronym for Cache Coherent UMA, because the
		hardware ensures that all the processor can see a memory modification
		performed by one of them.
		\item NUMA (Non Uniform Memory Access): Usually different groups
		of processors (SMP, Symmetric multiprocessors\footnote{Group of processors connected via buses. Usually they consist of not more than 32 processors.}) are connected, and processors belonging to different SMP can access memoryspaces of each others. As NUMA if is present a cache coherence mechanism this architecture is called CC-NUMA.
		\begin{figure}
			\centering
			\setlength{\fboxrule}{0.5pt}%
			\subfigure[UMA]{ \fbox{\includegraphics[scale=0.47]{./images/parallel_programming/shared_mem}}}
			\hfill
			\subfigure[NUMA]{\fbox{\includegraphics[scale=0.47]{./images/parallel_programming/numa}}}
			\hfill
			\caption{Shared memory architectures.}
			\label{fig:UMA_NUMA}
		\end{figure}
	\end{itemize}
	SM architecture provides an easy perspective to memory,	data sharing across processors, and parallelism since no explicit communication is involved. Memory access and communication are fast due to the proximity of memory to CPUs, but it is not scalable because adding more CPUs to the pool can geometrically increases the traffic on the bus and makes cache management harder. Is up to the programmer to ensure the correct accesses to global memory in order to avoid race-conditions.
	As an example of real SM modern processor Figure \ref{fig:intelPhi} shows the Intel \textit{Knights Landing} Architecture for the intel Phi processor family which is armed with 72 cores, $8$ billions transistors at \SI{14}{\nano\metre}, AVX-512 and is able to executes 240 threads simultaneously.
		\begin{figure}
		\centering
		\label{fig:distribuiteMemory}
		\includegraphics[scale=0.45]{./images/parallel_programming/xeonphi}
		\caption{Xeon Phi, Intel Knight Landing architecture and specifications.}
	\end{figure}
	Coupled with this architecture many software solution can be used to program
	shared memory machines. The most used are:
	\begin{itemize}
		\item Threads. Lightweight processes but with same PID (e.g. pthreads)
		\item A standard language with preprocessor directives to the compiler that is capable of converting the serial program in a parallel program without any (or very few) intervention by the programmer (e.g. \texttt{OpenMP}, see Section \ref{sec:openmp}).
		
	\end{itemize}
	

	\item [Distributed Memory] 	Different systems, and hence, different multiprocessors connected via some kind of network (see Figure \ref{fig:distribuiteMemory}), usually high speed networks such as gigabit Ethernet, InfiniBand and Myrinet, and the memory space in one processor do not map to another processor. Each of them operate independently on its memory address, so changes are not reflected on memory spaces of the others. Explicit communication is required between processors and is like synchronization	programmer's responsibility.
	This architecture  is very scalable and there is not  overhead in maintaining	cache coherency. 	
	The most used paradigm for programming distributed memory machines is the
	message passing\footnote{\texttt{MPI} is the \textit{de facto} industry standard for
	message passing. \url{http://www.mpi-forum.org/}}.
	\begin{figure}
		\centering
		\caption{Distributed memory architecture.}
		\label{fig:distribuiteMemory}
		\setlength{\fboxrule}{1pt}%
		\includegraphics[scale=0.25]{./images/parallel_programming/distribuitedMemory}
	\end{figure}
	
	\item[Hybrid Systems] As the name suggest is a mix of actitectures. Only a limited number of  processors, say $N$, have	access to a common pool of shared memory. These N processor are connected to the others via network and each processor can consist of many cores.
	A common example of a  programming model for hybrid system is the combination 	of the message passing model (\texttt{MPI}) with the threads model (\texttt{OpenMP}) in which
	\begin{inparaenum}[\itshape a\upshape)]
		\item threads perform computationally intensive task, using local
		\textbf{on-node} memory space and
		\item communications between processes on different nodes occurs over network
		using \texttt{MPI} (see figure \ref{fig:hybridMemory}).
	\end{inparaenum} 
\begin{figure}
	\centering
	\includegraphics[scale=0.8]{./images/parallel_programming/hybrid_model}
	\caption{Hybrid memory architecture (Each processor is milti-core)}
	\label{fig:hybridMemory}
\end{figure}
	
\end{description} 
\end{description}




\subsection{The \texttt{OpenMP} Parallel Computing Paradigm for Shared Memory Computers}
    \texttt{OpenMP} is a portable API providing compiler directives and library
    functions for shared memory parallel programming in \texttt{C/C++} and
    Fortran \cite{Chapman:2007:UOP:1370966} implemented on top of \texttt{pthread}. It implements the
    multi-threaded \emph{fork-join} (see Figure \ref{fig:omp_fork-join}) programming model, where an initial (or master) thread forks a given number of new threads (team of threads), which share the resources of the process which they are part of and run concurrently on the available processing
    elements. Threads created during the fork phase can therefore
    rejoin to the master thread (join phase), and more fork-join
    stages can occur in a typical execution of an \texttt{OpenMP} program.
\begin{figure}
	\centering
	\includegraphics[scale=0.8]{./images/parallel_programming/omp_fork-join}
	\caption{\texttt{OpenMP} fork-join execution model.}
	\label{fig:omp_fork-join}
\end{figure}


    The fork-join model allows for the selective parallelization of
    the original source code (e.g. loops), by leaving portions that
    are difficult to be parallelized or that would lead to negligible
    (or even worsening) improvements, unchanged with respect to the
    serial implementation. By default, iterations are equally
    subdivided in chunks and statically assigned to threads by a
    round-robin policy. However, iterations can also be assigned to
    threads on demand, by using the dynamic scheduling clause. In this
    manner, when a thread terminates its current chunk, it requests a
    new one in a typical master-slave manner, usually resulting in
    better performance in the case chunks are not well balanced.

    Moreover, \texttt{OpenMP} provides locks to serialize the access to shared
    variables by defining critical sections. A lock must be firstly
    initialized and then can be acquired or released. When a thread
    attempts to acquire a lock that is already be set by another
    thread, its execution is suspended until the lock is released,
    giving rise to performance degradation (or even to possible deadlock
    situations). However, a lock can also be queried in order to
    evaluate its state, without blocking the thread execution. In this
    way, if the lock is already set, the querying thread can perform
    other computation, by minimizing the idle time.

    In addition to the data-type parallelization provided by the
    fork-join model, \texttt{OpenMP} aslo supports the functional-type
    parallelization, where different portions (regions) of code to be
    processed are assigned to different threads. In both cases, \texttt{OpenMP}
    parallelization of a code is straightforward, by hiding most
    low-level implementation details. Moreover, by using \texttt{OpenMP} it is
    possible to build the same source code to produce both parallel or
    sequential executable. In the latter case, the compiler simply
    ignores the \texttt{OpenMP} directives.
    
    
    \subsection{General Purpouse GPU Computing - GPGPU}
    The concept of many processor working together in concert is not new in computer graphics. Since the demand generated by entertainment started to grow, multi-core hardware emerged in order to take advantage of the high amount of parallel work available during in the processo of generating and rendering 3D images.
    In computer graphics, the process of generating a 3D images consist of
    refreshing pixels at rate of sixty or more \SI{Hz}. Each has to be processed goes through a number of stages, and this process is commonly referred to as the \emph{graphic processing pipeline}. The peculiarity of this task is that the  computation each pixel is independent of the other's so this work is perfectly suitable for distribution over parallel processing elements. To support extremely fast processing of large graphics data sets (which mainly consists of vertices and fragments), modern GPUs employ a stream processing model with parallelism.
    The game industry boosted the development of the GPU, that offer now greater
    performance than CPUs and are improving faster too (see Figure
    \ref{CPU-VS-GPU_GFLOP} and \ref{CPU-VS-GPU_MEMORY}).
    The reason behind the discrepancy in floating-point capability between CPU and  GPU is that GPU is designed such that more transistors are devoted to data  processing rather than caching and flow control.
    
    \begin{figure}
    	\centering
    	\includegraphics[scale=0.4]{./images/parallel_programming/memory-bandwidth}
    	\caption{Intel CPUs and Nvidia GPUs memory bandwidth
    		chart}\label{CPU-VS-GPU_MEMORY}
    \end{figure}
    \begin{figure}
    	\centering
    	\includegraphics[scale=0.3]{./images/parallel_programming/cpu-vs-gpu}
    	\caption{Performance comparisong between CPU and modern accelerators (NVIDIA, Intel and AMD) over time.}\label{CPU-VS-GPU_GFLOP}
    \end{figure}
    Nowadays, GPU are widely used for general purpouse computing. The Top 500 Supercomputers\footnote{\url{http://www.top500.org/statistics/list/}} ranking is dominated by massively parallel computer, built on top of superfast networks and millions of sequential CPUs working in concert but as the industry is developing even ore powerful, programmable and capable GPUs in term of GFlops  we see that they begin to offer advantages over traditional cluster of computers in terms of economicity and scalability
    
    \subsection{The \texttt{OpenCL} Parallel Computing Paradigm on Heterogeneous Devices}
    \texttt{OpenCL} (acronym for Open Computing Language) is an open framework for parallel programing on heterogeneous devices. Among \texttt{OpenCL} compliant devices, GPUs are particularly interesting for their elevated
    computational performances (a modern GPU is capable of executing
    thousands of independent threads in parallel) and cheapness (if
    compared to other many-core coprocessors and high-performance
    devices). For instance, a Nvidia GTX 680 GPU (Kepler Architecture)
    consists in a number of 8 SIMD (Single Instruction, Multiple Data)
    multiprocessors (or compute units), each one made by 192 CUDA
    cores, for a total of 1,536 concurrent running threads.
    
    A typical \texttt{OpenCL} application is subdivided in two parts, one
    running on the CPU (host application) and one running on a
    compliant device (device application), where the actual parallel
    computation generally takes place. The host application defines
    the tasks to be executed in parallel. Each parallel task is
    implemented as an \texttt{OpenCL} \emph{kernel}, which is a special C99
    function, and deployed to a compliant device (or even to different
    compliant devices) for execution. Each kernel is executed by a
    team of threads, also called work-items, which are grouped into
    workgroups. Data to be processed has to be explicitly partitioned
    and assigned to compute units so that each work-item runs the same
    kernel on different portions of data in a SIMD fashion. At this
    purpose, a unique identifier (ID) is associated to each
    work-item. For example, in case of an array of $n$ elements and
    $n$ work-items, data can be partitioned by associating each work
    item to the array element with index corresponding to the
    work-item ID. Moreover, a local ID is defined for each work-item
    within a workgroup. When the kernel execution terminates,
    work-items globally synchronize, and the control returns to the
    host application. Similarly to the OpenMP fork-joins stages,
    different kernel executions and synchronization stages can take
    place in a typical \texttt{OpenCL} application.
    
    Data can be shared by all the running threads by means of the
    device global memory, which is generally the larger among the
    different memory levels available on the device (e.g. on GPUs)
    though being the slower one. A read-only memory, equivalent to the
    global one in terms of latency and dimension, called constant
    memory, is also available. Some devices have an appropriate
    portion of this memory, while in other cases the constant memory
    space coincides with that of the global memory. Threads within a
    workgroup are executed by a specific compute unit and therefore
    can share data on the local memory and also synchronize each
    other. Local memory is generally smaller with respect to the
    global one, but allows for faster access (about $100\times$ faster
    on modern GPUs). Eventually, each work-item has its own private
    memory, which is at the same time the fastest and the smaller
    one. The memory space in which a given data must be stored must be
    initially defined by the host. However, data can move among
    different memory levels during kernel execution.
    
    Data exchange and kernels execution are managed host-side thanks
    to an \texttt{OpenCL} context. In particular, the host application links
    kernels into one or more containers, called \emph{programs}. The
    program therefore connects kernels with the data to be processed
    and dispatches them to a special \texttt{OpenCL} structure called
    \emph{command queue}. This is necessary because only enqueued
    kernels are actually executed. The context contains all the
    devices, command queues and kernels, each device has its own
    command queue and each command queue contains the kernels to be
    executed on the corresponding device. Moreover, an \texttt{OpenCL}
    application can configure different devices to perform different
    tasks, and each task can operate on different data. \texttt{OpenCL}
    provides thus a full task-parallelism. Command queues are also
    used for host-device and device-device data transfer operations,
    synchronization between different kernels, and profiling
    operations.
    